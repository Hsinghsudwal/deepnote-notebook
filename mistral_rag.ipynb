{"cells":[{"cell_type":"code","metadata":{"cell_id":"14b0004286d24661bd5a0dbe70d65549","deepnote_cell_type":"code"},"source":"# %pip install pypdf mistralai faiss-cpu","block_group":"eefc9fc15438408abd87de0bd4bfada8","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"bbecd7427a5e461daba4c534cb5efc9a","deepnote_cell_type":"markdown"},"source":"## Read pdf\n","block_group":"f7a2b42e87a7448ab82859217a37c9fe"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"4fb2207bdff64f1aa5da03fb7b806c97","deepnote_cell_type":"code"},"source":"from pathlib import Path\nfrom pypdf import PdfReader\n\n\npdf_files = Path(\"data\").glob(\"*.pdf\")\ntext = \"\"\n\nfor pdf_file in pdf_files:\n    reader = PdfReader(pdf_file)\nfor page in reader.pages:\n    text += page.extract_text() + \"\\n\\n\"","block_group":"d65d4ebe0bbb4ae2b76e9842fde78831","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"allow_embed":"code_output","cell_id":"9ec4a237cb874a0aae87ee464b3b080e","deepnote_cell_type":"code"},"source":"print(text[:100])\nprint(len(text))","block_group":"4d3d0a7132f5459b9a2c4851b8a5946d","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Benedict Neo\n/envel‚å¢pebenedict.neo@outlook.com /linkedinin/benedictneo /github@benthecoder\nEducation\n3817\n"}],"outputs_reference":"dbtable:cell_outputs/36894684-a856-40a4-82ef-fbd6f2f77e16"},{"cell_type":"markdown","metadata":{"cell_id":"d603f41e554047bab1b17cfbc2ce6db1","deepnote_cell_type":"markdown"},"source":"## chunking\n","block_group":"b1a3ae57e28643e2b1381afc937cff42"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"43863a9cdf8b4fddbc700242978b4721","deepnote_cell_type":"code"},"source":"chunk_size = 500\nchunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\nlen(chunks)","block_group":"bb469994a3e04e7dabc94ed943dea42d","execution_count":null,"outputs":[{"data":{"text/plain":"8"},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"dbtable:cell_outputs/2541b861-a0a4-4c32-bb9b-b07f30705be5"},{"cell_type":"markdown","metadata":{"cell_id":"1662ed57271e4e39a51eb2e98b5b80ce","deepnote_cell_type":"markdown"},"source":"in RAG, we need to split documents into smaller chunks so it's more effective to identify and retrieve teh most relevant information\n\ndepending on the use case, a smaller chunk size will be beneficial for RAG to identify and extract relevant information more accurately, as larger text chunks can contain filler text that obscures the semantic representation\n\nHere, we combine 500 characters into one chunk, and we get 8 chunks.\n","block_group":"8949d65df6ea416ab2c726a63c5da763"},{"cell_type":"markdown","metadata":{"cell_id":"b55a0fc087e9457a934e0c86f6384c4f","deepnote_cell_type":"markdown"},"source":"## embed\n","block_group":"3ab474c2c9054088ac3d79f8d61038fc"},{"cell_type":"markdown","metadata":{"cell_id":"51fc1777a55a49be8a8102906de91d4e","deepnote_cell_type":"markdown"},"source":"for each text chunk, we create a text embedding, which are numerical representations of text in the vector space.\n\nWords with similar meanings are closer to each other in this space.\n\nTo create embeddings, we use Mistral AI's embeddings API endpoint.\n\nWe create a simple embed function to get embeddings from a single chunk, store all of them in a numpy array\n","block_group":"7f3fd158f2cf42888d6d0c33fcb0d064"},{"cell_type":"code","metadata":{"allow_embed":"code_output","cell_id":"503aebd913d743108ce85be97c5e9c65","deepnote_cell_type":"code"},"source":"from mistralai.client import MistralClient\nimport numpy as np\n\nclient = MistralClient(api_key=\"YOUR_MISTRAL_KEY\")\n\n\ndef embed(input: str):\n    return client.embeddings(\"mistral-embed\", input=input).data[0].embedding\n\n\nembeddings = np.array([embed(chunk) for chunk in chunks])\ndimension = embeddings.shape[1]","block_group":"84b8cf81d0294033907fc53d16ba5ca0","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"612c86ebd2a94fdf84a2c35d17a90549","deepnote_cell_type":"code"},"source":"embeddings","block_group":"1290cadf16c14c079affc05bf292501b","execution_count":null,"outputs":[{"data":{"text/plain":"array([[-0.03314209,  0.03010559,  0.03341675, ..., -0.00093889,\n         0.03649902,  0.01657104],\n       [-0.03050232,  0.06610107,  0.06039429, ..., -0.02508545,\n        -0.00403595, -0.02178955],\n       [-0.02389526,  0.06365967,  0.04605103, ..., -0.02262878,\n        -0.00494003, -0.02874756],\n       ...,\n       [-0.02015686,  0.03216553,  0.04882812, ..., -0.01455688,\n         0.00720596, -0.0216217 ],\n       [-0.00782013,  0.043396  ,  0.0413208 , ..., -0.02740479,\n        -0.002491  , -0.02764893],\n       [-0.0513916 ,  0.03872681,  0.03271484, ..., -0.01724243,\n         0.03497314, -0.01080322]])"},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"dbtable:cell_outputs/cac17925-cc6c-4afe-a9a4-bba9e68aef51"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"ad1a897997dc4f599b5e7519dc704e9e","deepnote_cell_type":"code"},"source":"dimension","block_group":"d239780b30c445e4bbff726564d03918","execution_count":null,"outputs":[{"data":{"text/plain":"1024"},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"dbtable:cell_outputs/e667a91a-b1f3-42b2-912c-9af2d0cb512b"},{"cell_type":"markdown","metadata":{"cell_id":"c710a88b46b64d47aca63052af355954","deepnote_cell_type":"markdown"},"source":"## vector db\n","block_group":"87bd2ecc25a244558bcf1dded914d91c"},{"cell_type":"markdown","metadata":{"cell_id":"f2c221acce8045a9a0a34e807e88a028","deepnote_cell_type":"markdown"},"source":"once we have the embeddings, we store them in a vector db for efficient processing and retrieval.\n\nHere we use Faiss, an open source vector db developed by Meta.\n\nwe create an index to store our embeddings.\n\nlook at the different indexes: https://github.com/facebookresearch/faiss/wiki/Faiss-indexes\n","block_group":"f49ed9b2e7f34f8397425e184009dea2"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"6bf679ae355c457d921870c49c529e4b","deepnote_cell_type":"code"},"source":"import faiss\n\nd = embeddings.shape[1]\nindex = faiss.IndexFlatL2(d)\nindex.add(embeddings)","block_group":"2efccedce24a49a797803d9976b0ad82","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"0636af8825f14dd4bd9eb39898b71e90","deepnote_cell_type":"markdown"},"source":"## query\n","block_group":"e58f44e2b3a74045ad4dcbcca63666a2"},{"cell_type":"markdown","metadata":{"cell_id":"2346db7e51cf4a91936de4cabb740c8e","deepnote_cell_type":"markdown"},"source":"when user asks a question, we create embeddings by using the same model as before.\n","block_group":"df6a7d452cb6483d8b3c17d403b4c239"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"e9f8d5d281f044f89f8c6ae74ab6e678","deepnote_cell_type":"code"},"source":"question = \"Who is Benedict Neo?\"\nquestion_embeddings = np.array([embed(question)])","block_group":"2b0f04cf9b274cb3829be33e708df5fd","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"f3bd12d7e21f4e29be4f5d6279905cb9","deepnote_cell_type":"markdown"},"source":"## retrieval\n","block_group":"7e5d64ffe0c447feba0960388ea79b42"},{"cell_type":"markdown","metadata":{"cell_id":"a53108af90a4403ea3b76f959023f12f","deepnote_cell_type":"markdown"},"source":"we perform search on our vector db using `index.search`, it takes two parameters, the embedding of our question and k, which is the number of similar vectors to retrieve\n\nThe function returns the distances (D) and indices (I) of the most similar vector, and based on the indices, we can return the actual text.\n","block_group":"2c3f24a85485425bb9014525f45b9a2e"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"5f966a2c97a9423b80ee337d9b7e17b4","deepnote_cell_type":"code"},"source":"D, I = index.search(question_embeddings, k=2)  # distance, index\nretrieved_chunk = [chunks[i] for i in I.tolist()[0]]","block_group":"81d0a73a1c6b4832b5afe4ca0459a4c4","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"a7ddc589bea543ffb4eb3252818c0164","deepnote_cell_type":"markdown"},"source":"## Create prompt\n","block_group":"3f8e92f27b2649c6aa9fb089243bff05"},{"cell_type":"markdown","metadata":{"cell_id":"abcda611454c4032837087ece98d2333","deepnote_cell_type":"markdown"},"source":"we create a prompt template that combines the chunk and the question\n","block_group":"e3d7f6718b554b1f9483b01d07a1e126"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"c3386012001845498fbc8f181b7dca88","deepnote_cell_type":"code"},"source":"prompt = f\"\"\"\nContext information is below.\n---------------------\n{retrieved_chunk}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {question}\nAnswer:\n\"\"\"","block_group":"7fe48d0a6e674ad4b734991ee7890842","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"ffcd1a7c798b4eb4961402f713b86c19","deepnote_cell_type":"markdown"},"source":"## Chat model\n","block_group":"5ca33d28cfb547029f8793eedf8390ff"},{"cell_type":"markdown","metadata":{"cell_id":"ccb7f04288b8410a97cbaebfc0c4e17a","deepnote_cell_type":"markdown"},"source":"using the mistral chat completion API with a mistral model, here we're using `mistral-medium`, we generate an answer based on the user question and the context retrieved\n","block_group":"69913c0ac100412a9341021eaec0ad19"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"5ec91eaaeac14acaa11e359250adf832","deepnote_cell_type":"code"},"source":"from mistralai.client import ChatMessage\n\n\ndef run_mistral(user_message, model=\"mistral-medium\"):\n    messages = [ChatMessage(role=\"user\", content=user_message)]\n    chat_response = client.chat(model=model, messages=messages)\n    return chat_response.choices[0].message.content\n\n\nrun_mistral(prompt)","block_group":"13a4ca45967f4191b5913ba2404e82fb","execution_count":null,"outputs":[{"data":{"text/plain":"'Benedict Neo is a current student at Iowa State University pursuing a Bachelor of Science in Statistics with a minor in Computer Science, expected to graduate in December 2023. He has a GPA of 3.95. His coursework includes Experimental Design, Bayesian Statistics, Design & Analysis of Algorithms, and Large-scale Data Analysis.\\n\\nBenedict has worked as an Undergraduate Research Assistant at Iowa State University since January 2022. In this role, he led the development of the WEPPR R package, improved search speed by implementing concurrent processing in FastAPI, and built Svelte components to render search results in a user-friendly design.\\n\\nHis skills include programming languages such as Python, R, SQL, SAS, JavaScript (React), Java, HTML/CSS, and Bash. He is also proficient in libraries such as Pandas, NumPy, Matplotlib, Plotly, Tidyverse, Scikit-Learn, NLTK, PyTorch, and PySpark, and tools such as AWS, Google Cloud, Docker, Power BI, Tableau, Git, Linux, Hadoop, Spark, and Airflow.\\n\\nBenedict can be contacted via email at [envelope](mailto:benedict.neo@outlook.com)benedict.neo@outlook.com or LinkedIn at linkedin.com/in/benedictneo. He also has a GitHub profile at github.com/benthecoder.'"},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"s3:deepnote-cell-outputs-production/464323f8-3c0f-443d-a94a-700d7bc9fe77"},{"cell_type":"markdown","metadata":{"cell_id":"8ea464ea38204805862950fa047f6e9a","deepnote_cell_type":"markdown"},"source":"## all together now\n","block_group":"b6b388cdfc1948f094b2864433d01d62"},{"cell_type":"code","metadata":{"allow_embed":"code_output","cell_id":"3e96df3dcf6a41289b49530ad8ef325c","deepnote_cell_type":"code"},"source":"from faiss import IndexFlatL2\n\nprompt = \"\"\"\nContext information is below.\n---------------------\n{context}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nQuery: {query}\nAnswer:\n\"\"\"\n\n\ndef ask(query: str, index: IndexFlatL2, chunks):\n    embedding = embed(query)\n    embedding = np.array([embedding])\n\n    _, indexes = index.search(embedding, k=2)\n    context = [chunks[i] for i in indexes.tolist()[0]]\n\n    user_message = prompt.format(context=context, query=query)\n\n    messages = [ChatMessage(role=\"user\", content=user_message)]\n    chat_response = client.chat(model=\"mistral-medium\", messages=messages)\n    return chat_response.choices[0].message.content\n\n\nask(\"What work experience does he have?\", index, chunks)","block_group":"06f000cec10342988f837b7f5e48d1e0","execution_count":null,"outputs":[{"data":{"text/plain":"\"Based on the provided context information, the individual has experience as a President of the Google Developer Student Club at Iowa State University from August 2022 to May 2023. In this role, they led a team of 12 core officers to organize and host tech workshops and talks for over 100 students.\\n\\nAdditionally, they have experience as a Data Analyst Intern at Tesla from May 2022 to August 2022. Their responsibilities at Tesla included architecting and deploying Airflow ETL pipelines with Docker on Linux, designing and optimizing MySQL database schemas, developing Python packages interfacing with various APIs, and building an interactive web app with Streamlit for failure analysis using text embeddings and NLP techniques.\\n\\nIt's worth noting that the context information does not specify the duration of the individual's experience as a Data Analyst at a company with revenue up to $1 Million/month, but it can be inferred that they have experience with data visualization, hypothesis testing, regression, classification, NLP, and time series analysis based on the context.\""},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"s3:deepnote-cell-outputs-production/f331eaa0-9d73-48cf-b274-95a119e8ad81"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"409771f2f40048a2b2d97fade238aa76","deepnote_cell_type":"code"},"source":"ask(\"Does he know how to code in Python?\", index, chunks)","block_group":"33cba9b079c7496ea6e859dcc1b9d185","execution_count":null,"outputs":[{"data":{"text/plain":"'Yes, the individual listed Python as one of the languages they know how to code in.'"},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"dbtable:cell_outputs/ed0cca12-bef5-4301-9eb1-223812f46d7f"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"8e0200f866004eebaf71fb97dc155797","deepnote_cell_type":"code"},"source":"ask(\"What projects has he worked on?\", index, chunks)","block_group":"faf85992c55e44dfa2b4219235078d61","execution_count":null,"outputs":[{"data":{"text/plain":"'The person has worked on two projects based on the provided context information.\\n\\n1. A project related to analyzing work orders text using NLP and machine learning algorithms for categorizing failures. Additionally, they developed 10 PowerBI dashboards, optimized data models using DAX queries, and analyzed shift hours and badging data for peak hour optimization.\\n2. A GitHub project called \"githubClassGPT\" that involved Python, LlamaIndex, LangChain, AWS S3, OpenAI, Docker, and Streamlit. They developed a chatbot with Netlify and Cloud Run, improved search speed using concurrent processing in FastAPI, and built Svelte components to render search results in a user-friendly design.'"},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"dbtable:cell_outputs/01c943b3-0b55-4dd5-9c4b-bc1e240d631c"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"7b4aa4d07a034010a2b357ab37d08d46","deepnote_cell_type":"code"},"source":"ask(\"Is he on the job market?\", index, chunks)","block_group":"d9fdaea143444d4e8d486b1466ecb4b4","execution_count":null,"outputs":[{"data":{"text/plain":"'Based on the provided context information, it is not explicitly stated whether the individual is currently on the job market or not. The information only mentions their skills, experiences, and past leadership roles. To determine their employment status, additional context or information is needed.'"},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"dbtable:cell_outputs/fc1e673a-31da-43f7-831a-b390eed45a62"},{"cell_type":"markdown","metadata":{"cell_id":"303e149cfa8240e882c7834aa5dfd3c1","deepnote_cell_type":"markdown"},"source":"## Streamlit\n","block_group":"793d1c1fd466462bb9716ccc282e3660"},{"cell_type":"markdown","metadata":{"cell_id":"a09392a9f7ff4883a5938bff2bf32e6a","deepnote_cell_type":"markdown"},"source":"### building the index\n","block_group":"bcdac4c3a6b542e98a9ff27c4774628b"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"a683fd18d78b485e9fe103fa08bf928e","deepnote_cell_type":"code"},"source":"# Function to build and cache the index from PDFs in a directory\n@st.cache_resource\ndef build_and_cache_index():\n    \"\"\"Builds and caches the index from PDF documents in the specified directory.\"\"\"\n    pdf_files = Path(\"data\").glob(\"*.pdf\")\n    text = \"\"\n\n    for pdf_file in pdf_files:\n        reader = PdfReader(pdf_file)\n        for page in reader.pages:\n            text += page.extract_text() + \"\\n\\n\"\n\n    chunk_size = 500\n    chunks = [text[i : i + chunk_size] for i in range(0, len(text), chunk_size)]\n\n    embeddings = np.array([embed(chunk) for chunk in chunks])\n    dimension = embeddings.shape[1]\n    index = IndexFlatL2(dimension)\n    index.add(embeddings)\n\n    return index, chunks","block_group":"7c63d56c04f246acbc7d6fb934047ffc","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"f4b80448f8944e5c809afc1aa7a81494","deepnote_cell_type":"markdown"},"source":"### Streaming\n","block_group":"eb0f3c7b20b546e885367202af572d95"},{"cell_type":"code","metadata":{"allow_embed":"code_output","cell_id":"21bc1f732faf46048abe086a85cf99af","deepnote_cell_type":"code"},"source":"# Function to stream a string with a delay\ndef stream_str(s, speed=250):\n    \"\"\"Yields characters from a string with a delay to simulate streaming.\"\"\"\n    for c in s:\n        yield c\n        time.sleep(1 / speed)\n\n\n# Function to stream the response from the AI\ndef stream_response(response):\n    \"\"\"Yields responses from the AI, replacing placeholders as needed.\"\"\"\n    for r in response:\n        content = r.choices[0].delta.content\n        # prevent $ from rendering as LaTeX\n        content = content.replace(\"$\", \"\\$\")\n        yield content","block_group":"cff1c63ef3184fe293f68503c8582f30","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"d2ba7674be7d4066a329904c58207dc8","deepnote_cell_type":"markdown"},"source":"### messages\n","block_group":"9f9c2233e4d84bcfb016fc7d308a82fd"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"10a8bbd07a6b4fcc8a4e2fd6e759cb1d","deepnote_cell_type":"code"},"source":"# Function to add a message to the chat\ndef add_message(msg, agent=\"ai\", stream=True, store=True):\n    \"\"\"Adds a message to the chat interface, optionally streaming the output.\"\"\"\n    if stream and isinstance(msg, str):\n        msg = stream_str(msg)\n\n    with st.chat_message(agent):\n        if stream:\n            output = st.write_stream(msg)\n        else:\n            output = msg\n            st.write(msg)\n\n    if store:\n        st.session_state.messages.append(dict(agent=agent, content=output))","block_group":"46d6c7bef53b4f35ac1da152134d509d","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"3e23e291692d42d0abd11574737890ff","deepnote_cell_type":"markdown"},"source":"### main\n","block_group":"f25f94362f7348039f748a949e2c00fe"},{"cell_type":"code","metadata":{"allow_embed":true,"cell_id":"9d21838800884856913f0eb9a1505aa8","deepnote_cell_type":"code"},"source":"# Main application logic\ndef main():\n    \"\"\"Main function to run the application logic.\"\"\"\n    if st.sidebar.button(\"üî¥ Reset conversation\"):\n        st.session_state.messages = []\n\n    index, chunks = build_and_cache_index()\n\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"agent\"]):\n            st.write(message[\"content\"])\n\n    query = st.chat_input(\"Ask something about your PDF\")\n\n    if not st.session_state.messages:\n        add_message(\"Ask me anything!\")\n\n    if query:\n        add_message(query, agent=\"human\", stream=False, store=True)\n        reply(query, index, chunks)","block_group":"96378b09659041e69ea5dc28182566aa","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a605a3e6-1564-47b2-94e7-842290ba7692' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"dbd8fc2a90cc4d10bb7bda98e204ba21","deepnote_execution_queue":[]}}